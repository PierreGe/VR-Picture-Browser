\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{lscape}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{afterpage}
\author{Pierre Gerard - Matteo Marra - Bruno Rocha Pereira}
\title{Given One \\ Intelligent Picture Browser \\ Requirements and Analysis report}
\date{December 19, 2016}

\begin{document}

\maketitle

\section{Introduction}

New generation user interfaces are developing day by day, with new tools, devices and different use cases showing up.

Of those devices, Virtual Reality sets are expanding as those devices get more affordable and many research team started working on it. 

Our brainstorming led to us imagining a future where we would use Virtual Reality on common basis, we thought about what an user would like to have and that doesn't exist yet, and how to implement it to make the user feel comfortable in this totally new environment. 

The system will offer the user a collection of pictures in a Virtual Reality environment, allowing him to browse them intelligently.

\section{Problem to be solved}

Since the numeric revolution, humans tend to take a ton of pictures. It is especially true during their holidays, usually coming back with thousands of pictures. The problem that arise then is that human don't usually have a mean to explore them all other than browsing through them one by one. So, the idea for the project is to create a \textit{Intelligent Picture Browser (IPB)} to fill that void.

\section{Requirement Analysis} \label{requi}

The product should browse the user complete gallery of pictures and select a subset of them. The subset selection will be based on a keyword/tag selection. The keyword/tag is automatically linked to pictures by a AI algorithm.

The user will be able to interact with the interface trough the virtual reality set and be able to select tag and some settings via voice.

\subsection{Data requirements}

The interface should work with any set of pictures in a \textit{jpg} format. However for testing and demonstration purpose, a preloaded set of 300 holiday pictures will be available

\subsection{User characteristics}

The interface will be design to fit almost every individual having the capacity to use a virtual reality set and voice recognition. It will target technological novice as well as professional. Unfortunately, people with disabilities preventing them from using a VR set or voice recognition won't be able to use our system.

\subsection{Usability goals}

The usability of this project is going to be as much as straightforward as possible and will not require any particular skill or educational background. However having a regular access to technologies and computers will be a plus for a smoother use. In brief, it should be easy to learn, efficient and effective.
% requirement on our ngui in particukar

\subsection{User experience goals}

The main goal is to make the user relive the moment immortalized by the pictures and helping him remember the event and feel the same emotion once again. % user should feel that it is useful enjoyable, engaging and FUN

\section{Design and prototyping}

For the design, implementation and validation of the next generation user interface, we worked in an iterative way.
The idea was to create multiple evolutionary prototyping. Each prototype was then presented to the class. With such a method, we were able to create a final product on time and validated its usability.

\subsection{Iteration 1 : problem defined}

The first iteration consisted mainly on sitting in a comfortable room and brainstorming about which next-generation interface to design. Many idea had emerged and the one that convince us the most was to innovate in the picture browsing field. Indeed, multiple technologies have recently appears on the market and we though they could be use to enhance usability and user experience of such systems.
The first and main assumption made on behalf of the user is that the user possess a lot of picture but don't have the time of to browse or sort them all. We want to solve this user's problem.
The added value of our idea to traditional iterative picture browser was an intelligence which recognize effortlessly the content of user's pictures and immersive way to browse them thanks to new immersive technology known as virtual reality and voice recognition.

\subsection{Iteration 2 : requirements defined}

This iteration was about planning the realization of the idea for the interface and getting the requirements right. We presented the requirements in section \ref{requi}. The planning consisted of a gant chart with a general idea of the timeline we were going to follow for the next 3 months. Of course, due to unpredictable challenges some part got delayed and some moved quicker than expected.

% insert that gant chart
\subsection{Iteration 3 : low-fidelity prototype}

This iteration was all about making sure the product is feasible and that requirements could be reached by making a low-fidelity prototype with all components working separately.
The idea here was to find solution for each of those components :
\begin{itemize}
	\item A Neural Network capable of automatically tagging the image,
	\item A framework to support the HTV Vive Virtual reality set,
	\item A speech recognizer interoperable with the chosen VR framework,
\end{itemize}

More information about the technologies used can be find in architecture section \ref{techntechno} .

\subsection{Iteration 4 : high fidelity prototype}

The goal of this iteration was to managed to build a working high-fidelity prototype were the main feature of the interface would be there.
The prototype build at this point was capable of handling voice recognized keywords and showed the corresponding image in the Virtual Reality environment.

\subsection{Iteration 5 : prototype improvement}

This iteration focused on user goals and on usability trough improvement of the first high-fidelity prototype. Following users feedback, we changed the way the picture are displayed : instead of showing a "wall" of pictures we surround the user with picture, making a circle of pictures around the user.
We also added more advanced queries, where a user can do the union and the intersection of subsets (tags) of pictures.

\subsection{Iteration 6 : final product}

This iteration was about finishing and polishing our user interface. We first added visibility to the features implemented and that a user can possibly use. To do that we added to tag suggestion and selected tag to the screen.

We also added feedback to the user. When he looks a pictures it get slightly bigger making him knows which one is currently selected.

We also map the Vive "hand" controller to movement inside the interface which are going upward and downward. We also added the possibility to move the image closer or farther by saying those keyword. 

Finally, we also made the evaluation of the interface presented in \ref{eval}.

\section{Technical : Intelligent Picture Browser}
The Intelligent Picture Browser is built to work on state of the art technology, in order to offer to the user a new kind of interaction.
It uses Virtual Reality for displaying and partially interacting, a neural network to categorize the pictures and speech recognition for selecting them.

It has a multimodal interface, that involves visualization, speaking and movement.
Other than interacting with the speech recognition, some commands are linked to the physical controllers of the HTC vive, that permits, in those cases, to have faster interaction.

\subsection{Functionality}

The user can select a tag and the system will show the picures associated.
He can select subsets of tags via an \textit{and} query, or he can do the union of two subsets with an \textit{or} query.
It can select a picture by just looking at it, and it can ask to the system to show the pictures closer or further, that it's, at the end, a zoom. All those interactions are done via voice recognition.

The user can move around, if the space around him allows, and get physically closer to the pictures to analyze them better.
Since the pictures are shown in a sort of cylinder all around the user, he might need to go up or down inside this cylinder in order to see partially hidden pictures. To do so, he can use the pad of the HTC vive controller, that will offer immediate feedback on the pressure of up or down. With the use of the controller the user can also rotate the selected picture by 90Â°.

Whenever the user looks on the ground, he can find a set of suggestions. Those suggestions are selected basing on the tags of the pictures shown, allowing the user to find similar tags that might show similar pictures.
An help system is also provided when \textit{help} is pronounced, offering the user support on how to activate the functionalities.

\subsection{Interactions}

% voice recognition for tag and non-vive-command intuite interaction
% for intuitive interaction

\subsection{Architecture} \label{techntechno}


The selected technologies are :
\begin{itemize}
	\item Google machine learning library TensorFlow for the neural network,
	\item Microsoft .net speech recognition library for the voice recognition
	\item Unity3D for the HTC Vive Virtual Reality environment.
\end{itemize}


% NN for tag -> json

% Utinty interface <- json
% unity interface <- win10 speech recognizer

% scripts used? 
% internal representation of the images (dictionary, paths, tags)


\subsection{Challenges}

% use unity and all its specificities

% poor software interoperability, as a result our project only work on win 10

% speech recognition is bad

% making the function visible

% VR new and doc is bad

% require a monster computer

\section{Evaluation} \label{eval}

In addition to continuous feedback from the teaching team and others student, we did a formal evaluation.

This evaluation aim is twofold; first assess the usability and assess the user experience.

Dealing with limited means, our evaluation is limited to three volunteers evaluators with no previous knowledge of our product.


% Student in CS and non-tech guy with no previous knwoledge on our product.

% Evalution take place in front of computer in natural setting


\subsection{Usability}

\subsubsection{Conducting evaluation}

The evaluation is a discount evaluation of 3 users using Nielsen Heuristic. According to Nielsen and Landauer (1993), this evaluation should allows us to discover about 60 percents of usability problems. Also according to Nielsen and Landauer the cost to benefit ratio of having 3 users is nearly maximal just under a ratio of 60.

% how it was done

\subsubsection{Results}

\subsection{User experience}


\subsubsection{Conducting evaluation}

\subsubsection{Results}



\section{Acknowledgement}

Before ending this report, we would really like to thanks the VUB Soft Lab for lending us the HTC Vive needed for the realization of this project.
We also would like to thanks the participant of the evaluation and all that give us feedback during the making of this Next-gen UI.

\section{Conclusion}


\end{document}




