\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{lscape}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{afterpage}
\author{Pierre Gerard - Matteo Marra - Bruno Rocha Pereira}
\title{Given One \\ - \\Intelligent Picture Browser \\  - \\Requirements and Analysis report}
\date{December 19, 2016}

\begin{document}

\maketitle

\section{Introduction}

New generation user interfaces are developing day by day, with new tools, devices and different use cases showing up.

Among those devices, Virtual Reality sets are spreading as they get more affordable and many research teams start working on it. 

Our brainstorming led us to imagine a future where we could use Virtual Reality on a common basis. We thought about what an user would like to have which doesn't exist yet, and how to implement it to make the user feel comfortable in this totally new environment.

The system will offer the user a collection of pictures in a Virtual Reality environment, allowing him to browse them in a smart way.

\section{Problem to be solved}

Since the numeric revolution, humans tend to take a ton of pictures. It is especially true during their holidays, usually coming back with thousands of pictures. The problem that arises then is that humans don't usually have a mean to explore them all other than browsing through them one by one. Therefore, the idea for the project is to create a \textit{Intelligent Picture Browser (IPB)} to fill that need.

\section{Requirement Analysis} \label{requi}

The product should browse the user's complete gallery of pictures and select a subset of them. The subset selection will be based on a keyword/tag which is automatically bound to pictures by an AI algorithm.

The user will be able to interact with the interface through the virtual reality set and be able to select tag and some settings via voice.

\subsection{Data requirements}

The interface should work with any set of pictures in a \textit{.jpg} format. However, for testing and demonstration purpose, a pre-loaded set of 300 holiday pictures will be available.

\subsection{User characteristics}

The interface will be design to fit almost every individual having the capacity to use a virtual reality set and voice recognition. It will target technological novice as well as professional. Unfortunately, people with disabilities preventing them from using a VR set or voice recognition won't be able to use our system.

\subsection{Usability goals}

The usability of this project is going to be as straightforward as possible and will not require any particular skill or educational background. Nonetheless, having a regular access to technologies and computers will bring smoother use. In brief, it should be easy to learn, efficient and effective.
% requirement on our ngui in particukar

\subsection{User experience goals}

The main goal is to make the user experience the moment immortalized by the pictures and allow him to feel the same emotions once again. % user should feel that it is useful enjoyable, engaging and FUN

\section{Design and prototyping}

Concerning the design, implementation and validation of the next generation user interface, we worked in an iterative way.
The idea was to create multiple evolutionary prototypes. Each prototype was then presented to the class. Thanks to that method, we were able to create a final product on time and validated its usability.

\subsection{Iteration 1 : Problem defined}

The first iteration consisted mainly in sitting in a comfortable room and brainstorming about which next-generation interface we would design. Many ideas had emerged and the one that convinced us the most was to innovate in the field of picture browsing. Indeed, multiple technologies have recently appears on the market and we though they could be use to enhance usability and user experience of such systems.
The first and main assumption made on behalf of the user is that the user possesses a lot of picture but doesn't have the time to browse or sort them all, which is what we want to solve.
The added value of our idea compared to existing picture browsers was an intelligence which recognizes effortlessly the content of the user's pictures and an immersive way to browse through them thanks to new technologies: virtual reality and voice recognition.

\subsection{Iteration 2 : Requirements defined}

The second iteration was about planning the realization of the idea for the interface and getting the requirements right. We presented the requirements in the section \ref{requi}. The planning consisted of a Gantt chart with a general idea of the timeline we were going to follow for the next 3 months. Of course, due to unpredictable challenges some parts of it got delayed and some of them were realized quicker than expected.

% insert that gant chart
\subsection{Iteration 3 : Low-fidelity prototype}

This iteration was all about making sure the product was feasible and that the requirements could be reached by making a low-fidelity prototype with all components working separately.

The idea here was to find solution for each of those components :
\begin{itemize}
	\item A Neural Network capable of automatically tagging the images,
	\item A framework to support the HTV Vive Virtual reality set,
	\item A speech recognizer compatible with the chosen VR framework,
\end{itemize}

More information about the technologies used can be found in the architecture section \ref{techntechno} .

\subsection{Iteration 4 : High fidelity prototype}

The goal of this iteration was to manage to build a fully-working high-fidelity prototype including the main features of the interface.
The prototype built at this point was capable of handling voice-recognized keywords as well as showing the corresponding images to the user in the Virtual Reality environment.

\subsection{Iteration 5 : Prototype improvement}

This iteration focused on user goals and on usability through improvement of the first high-fidelity prototype. Following users feedback, we changed the way the pictures were displayed : instead of showing a "wall" of pictures we decided to surround the user with picture, circling the user with pictures.
We also added more advanced queries, where a user can do the union and the intersection of subsets of pictures using combination tags.

\subsection{Iteration 6 : final product}

This iteration was about finishing and polishing our user interface. We first added visibility to the features implemented and that a user can possibly use. To do that we added to tag suggestion and selected tag to the screen.

We also added feedback to the user. When he looks a pictures it gets slightly bigger making him aware of the currently selected .

We also mapped buttons the Vive controller to the up and down movements in the virtual world. We also added the possibility to move the image closer or further by saying the corresponding keywords. 

Finally, we also made the evaluation of the interface presented in section \ref{eval}.

\section{Technical : Intelligent Picture Browser}
The Intelligent Picture Browser is built to work on state of the art technologies, in order to offer to the user a new kind of interaction.
It uses Virtual Reality for displaying and partially interacting, a neural network to categorize the pictures and speech recognition for selecting them.

It has a multimodal interface, that involves visualization, speaking and movement.
Other than interacting with the speech recognition, some commands are linked to the physical controllers of the HTC vive, that permits, in those cases, to have faster interaction.

\subsection{Functionalities}
Functionalities are divided in two sections: basic and advanced.
The first one represent the basics of the application, the second ones are less intuitive operations that were added to complete the user experience.
After executing the tagging script, images are flagged with one or more tags, that are considered the most probable for them.

\subsubsection{Basic functionalities}

The user can:
\begin{itemize}
\item Select pictures using one of their tag
\item Navigate through the picture shown around him
\item Select a picture to see it bigger 
\item Look at suggestions of tags.
\item Open an helper that will explain how to activate the commands
\end{itemize} 
The tag suggestions are selected depending on the current shown pictures, trying to find similar tags present in most of them.
At the beginning, the suggested tags are the most present in all the pictures tagged.


\subsubsection{Advanced functionalities}
The user can
\begin{itemize}
\item Query different tags via mathematical intersection and union
\item Move the pictures closer and further (zoom)
\item Rotate the selected picture
\item Move up and down through the pictures
\end{itemize}

\subsection{Interactions}
Interactions with the applications are possible in four possible modes:
\begin{enumerate}
\item \textbf{Movement} Being in a Virtual Reality, the user can move around, if the space around him allows, and get physically closer to the pictures to analyze them better.

\item \textbf{Vision}
When a user looks at a picture, it gets automatically selected to allow the user to apply advanced functions.
Whenever he looks at the ground, he will always find suggestions for tags.

\item \textbf{Speech} The user can say a tag, and immediately pictures associated with that tag will be shown.
When the user says the keyword \texttt{help} a text of help is shown in front of him, so he can read the different commands he can say and execute.

When he says the keyword \texttt{and} or \texttt{or}, followed by a tag, the related mathematical operation will be executed on the set of the currently shown pictures and the set of pictures associated to the newly pronounced tag.
When he says \texttt{further} or \texttt{closer} the pictures are zoomed in or out according to the keyword said.

\item \textbf{Touch} Using the controller of the HTC-Vive the user can toggle different commands:
using the back trigger he can rotate of 90 degrees the selected picture, in order to improve its visualization.

Using the buttons up and down of the touchpad, he can move up and down in the cylinder of pictures that surrounds him, being able to see also the pictures too far from him.

\end{enumerate}

\paragraph{Note on visualization} The pictures are shown in the Virtual reality environment all around the user. 
This means that the user, in order to look at pictures, simply has to rotate.
If many pictures are shown, they form a sort of cylinder around the user, being displayed in a circular way around him at different height level. That's why it was necessary to add commmands to move up and down.

During the development of the project we tried out different layouts of pictures: in our first prototype, in fact, pictures were shown in a big plane in front of the user. This means that with a big number of pictures, the user would need to move left, right, up and down, since doing only one layers of pictures wouldn't have allowed to see many pictures at the same time.

Movements left-right are allowed in virtual reality, since the user can move and the sensor will track his movement. But big sets of pictures would have meant that the user had to move probably way over its possible space around him, since the HTC Vive has physical limitations due to the cable attached to the computer and to the room itself.
This is why we preferred to have the user surrounded by pictures, offering two buttons to move up and down that we would have needed to add anyway.

\paragraph{Note on the controller commands} The three different commands controller-activated were also implementable by voice query, as most of the other commands of the application.
We preferred a controller-approach because it gives a better immediate feedback than the voice recognition, and because we didn't want to overcharge our dictionary with other keywords that the user had to remember.
\subsection{Software Technologies} \label{techntechno}

The selected technologies are :
\begin{itemize}
	\item Google machine learning library TensorFlow for the neural network,
	\item Unity3D for the HTC Vive Virtual Reality environment.
	\item C$\#$ on \textit{mono} reduced set of Microsoft .net framework.
	\item Microsoft-Unity windows speech recognition library (\texttt{Unity.Windows.Speech})for the voice commands.
	\item Json format to store tags associated to a picture path.
	\item \texttt{Newtonsoft.Json} library to parse the Json in the C$\#$ environment.
\end{itemize}

\todo{Maybe add a some lines on tensorflow? @Pierre}
% NN for tag -> json

% Utinty interface <- json
% unity interface <- win10 speech recognizer
\subsection{Architecture}


% scripts used? 
% internal representation of the images (dictionary, paths, tags)


\subsection{Challenges}

% use unity and all its specificities

% poor software interoperability, as a result our project only work on win 10

% speech recognition is bad

% making the function visible

% VR new and doc is bad

% require a monster computer

\section{Evaluation} \label{eval}

In addition to continuous feedback from the teaching team and others student, we did a formal evaluation.

This evaluation aim is twofold; first assess the usability and assess the user experience.

Dealing with limited means, our evaluation is limited to three volunteers evaluators with no previous knowledge of our product.


% Student in CS and non-tech guy with no previous knwoledge on our product.

% Evalution take place in front of computer in natural setting


\subsection{Usability}

\subsubsection{Conducting evaluation}

The evaluation is a discount evaluation of 3 users using Nielsen Heuristic. According to Nielsen and Landauer (1993), this evaluation should allows us to discover about 60 percents of usability problems. Also according to Nielsen and Landauer the cost to benefit ratio of having 3 users is nearly maximal just under a ratio of 60.

% how it was done

\subsubsection{Results}

\subsection{User experience}


\subsubsection{Conducting evaluation}

\subsubsection{Results}



\section{Acknowledgement}

Before ending this report, we would really like to thanks the VUB Soft Lab for lending us the HTC Vive needed for the realization of this project.
We also would like to thanks the participant of the evaluation and all that give us feedback during the making of this Next-gen UI.

\section{Conclusion}


\end{document}




